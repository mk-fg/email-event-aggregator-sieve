#!/usr/bin/env python2
# -*- coding: utf-8 -*-
from __future__ import print_function

import itertools as it, operator as op, functools as ft
from os.path import basename, exists, expanduser, join
from collections import namedtuple, OrderedDict, deque
from contextlib import contextmanager, closing
import email, email.header, email.errors
import os, sys, re, types, struct, string, time, base64, hashlib, random, sqlite3

class Defs(object):
	conf_paths = '~/{}.conf.py'.format(
		basename(__file__).split('.', 1)[0] ), '~/.eeas.conf.py'
	db_path = '~/.eeas.db'
	default_verdicts = {False: 'filter', True: 'pass'}
	history_timeout = 7 * 24 * 3600 # 1 week
	history_cleanup_chance = 0.05

os.umask(0077)


def force_bytes(bytes_or_unicode, encoding='utf-8', errors='backslashreplace'):
	if isinstance(bytes_or_unicode, bytes): return bytes_or_unicode
	return bytes_or_unicode.encode(encoding, errors)

def force_unicode(bytes_or_unicode, encoding='utf-8', errors='replace'):
	if isinstance(bytes_or_unicode, unicode): return bytes_or_unicode
	return bytes_or_unicode.decode(encoding, errors)

def to_bytes(obj, **conv_kws):
	if not isinstance(obj, types.StringTypes): obj = bytes(obj)
	return force_bytes(obj)

def parse_timedelta(ts_str):
	from datetime import datetime, timedelta
	assert isinstance(ts_str, bytes), [type(ts_str), repr(ts_str)]
	ts_str = ts_str.replace('_', ' ')

	if ts_str.isdigit(): delta = timedelta(seconds=int(ts_str))
	else:
		_short_ts_days = dict(y=365.25, yr=365.25, mo=30.5, w=7, d=1)
		_short_ts_s = dict(h=3600, hr=3600, m=60, min=60, s=1, sec=1)
		def _short_ts_regexp():
			ts_re = ['^']
			for k in it.chain(_short_ts_days, _short_ts_s):
				ts_re.append(r'(?P<{0}>\d+{0}\s*)?'.format(k))
			return re.compile(''.join(ts_re), re.I | re.U)
		match = _short_ts_regexp().search(ts_str)
		if not match or not any(match.groups()): raise ValueError(ts_str)
		delta = list()
		parse_int = lambda v: int(''.join(c for c in v if c.isdigit()))
		for units in [_short_ts_days, _short_ts_s]:
			val = 0
			for k, v in units.iteritems():
				try:
					if not match.group(k): continue
					n = parse_int(match.group(k))
				except IndexError: continue
				val += n * v
			delta.append(val)
		delta = timedelta(*delta)
	return delta.total_seconds()


# Different from vanilla bencode in:
#  * Handling "leading zeroes" in keys (doesn't error - for cjdns compat)
#  * encode_none method (to "n")
#  * encode_string encodes unicode as utf-8 bytes

def _ns_class(cls_name, cls_parents, cls_attrs):
	for k, v in cls_attrs.viewitems():
		if isinstance(v, types.FunctionType):
			cls_attrs[k] = classmethod(v)
	return type(cls_name, cls_parents, cls_attrs)

class BTEError(Exception): pass

class Bencached(object):
	__slots__ = 'bencoded',
	def __init__(self, s): self.bencoded = s

class BTE(object):
	__metaclass__ = _ns_class

	unicode_enc = 'utf-8'
	enable_none = True
	enable_bool = True
	cjdns_compat = False

	def decode_int(cls, x, f):
		f += 1
		newf = x.index('e', f)
		n = int(x[f:newf])
		if x[f] == '-':
			if x[f + 1] == '0': raise ValueError
		elif x[f] == '0' and newf != f+1: raise ValueError
		return n, newf+1
	def decode_string(cls, x, f):
		colon = x.index(':', f)
		n = int(x[f:colon])
		if not cls.cjdns_compat\
			and x[f] == '0' and colon != f+1: raise ValueError
		colon += 1
		return (x[colon:colon+n], colon+n)
	def decode_list(cls, x, f):
		r, f = [], f+1
		while x[f] != 'e':
			v, f = cls.decode_func[x[f]](cls, x, f)
			r.append(v)
		return r, f + 1
	def decode_dict(cls, x, f):
		r, f = {}, f+1
		while x[f] != 'e':
			k, f = cls.decode_string(x, f)
			r[k], f = cls.decode_func[x[f]](cls, x, f)
		return r, f + 1
	def decode_none(cls, x, f):
		if not cls.enable_none: raise ValueError(x[f])
		return None, f+1
	decode_func = dict(l=decode_list, d=decode_dict, i=decode_int, n=decode_none)
	for n in xrange(10): decode_func[bytes(n)] = decode_string

	def encode_bencached(cls, x, r): r.append(x.bencoded)
	def encode_int(cls, x, r): r.extend(('i', str(x), 'e'))
	def encode_float(cls, x, r): r.extend(('f', struct.pack('!d', x), 'e'))
	def encode_bool(cls, x, r):
		if not cls.enable_bool: raise ValueError(x)
		if x: cls.encode_int(1, r)
		else: cls.encode_int(0, r)
	def encode_string(cls, x, r):
		if isinstance(x, unicode):
			if not cls.unicode_enc: raise ValueError(x)
			x = x.encode(cls.unicode_enc)
		r.extend((str(len(x)), ':', x))
	def encode_list(cls, x, r):
		r.append('l')
		for i in x: cls.encode_func[type(i)](cls, i, r)
		r.append('e')
	def encode_dict(cls, x, r):
		r.append('d')
		ilist = x.items()
		ilist.sort()
		for k, v in ilist:
			r.extend((str(len(k)), ':', k))
			cls.encode_func[type(v)](cls, v, r)
		r.append('e')
	def encode_none(cls, x, r):
		if not cls.enable_none: raise ValueError(x)
		r.append('n')
	encode_func = {
		Bencached: encode_bencached,
		unicode: encode_string,
		str: encode_string,
		types.IntType: encode_int,
		types.LongType: encode_int,
		types.FloatType: encode_float,
		types.ListType: encode_list,
		types.TupleType: encode_list,
		types.DictType: encode_dict,
		types.BooleanType: encode_bool,
		types.NoneType: encode_none,
	}

	def bdecode(cls, x):
		try: r, l = cls.decode_func[x[0]](cls, x, 0)
		except (IndexError, KeyError, ValueError) as err:
			raise BTEError('Not a valid bencoded string: {}'.format(err))
		if l != len(x):
			raise BTEError('Invalid bencoded value (data after valid prefix)')
		return r

	def bencode(cls, x):
		r = []
		cls.encode_func[type(x)](cls, x, r)
		return ''.join(r)


class EEASSigDB(object):

	_db_init = '''
		CREATE TABLE IF NOT EXISTS rate_limit_state (
			fingerprint BLOB NOT NULL,
			agg_name TEXT NOT NULL,
			tokens REAL NOT NULL,
			ts_sync REAL NOT NULL
		);
		CREATE UNIQUE INDEX IF NOT EXISTS
			agg_idx ON rate_limit_state (agg_name, fingerprint);

		CREATE TABLE IF NOT EXISTS meta (
			var TEXT PRIMARY KEY ON CONFLICT REPLACE NOT NULL,
			val TEXT NOT NULL
		);
	'''

	_db_migrations = [
		# Assuming sqlite might be lacking foreign keys (e.g. disabled during build)
		'''CREATE TABLE IF NOT EXISTS rate_limit_hits (
				fingerprint BLOB NOT NULL,
				agg_name TEXT NOT NULL,
				ts REAL NOT NULL );
			CREATE INDEX IF NOT EXISTS
				agg_idx_ts ON rate_limit_hits (agg_name, fingerprint, ts);''',
		'''CREATE INDEX IF NOT EXISTS ts_cleanup ON rate_limit_state (ts_sync);
				CREATE INDEX IF NOT EXISTS ts_cleanup_hits ON rate_limit_hits (ts);''',
		'ALTER TABLE rate_limit_hits ADD COLUMN pass INT NOT NULL DEFAULT 0;' ]

	_db = None

	def __init__(self, conf, log=None, commit_after=None):
		self._log, self._db = log, sqlite3.connect(conf.db_path, timeout=60)
		self._db.row_factory = sqlite3.Row
		self.conf = conf

		# commit_after should be a tuple of (queries, seconds)
		seq, ts = (None, None) if not commit_after else\
			((v if v and v>=0 else None) for v in commit_after)
		self._db_seq_limit, self._db_ts_limit = seq, ts
		self._db_seq, self._db_ts = 0, time.time()

		self._init_db()

	def __del__(self):
		if self._db:
			self._db.commit()
			self._db.close()
			self._db = None

	@contextmanager
	def _cursor(self, query, params=tuple(), **kwz):
		if self._log: self._log.debug('Query: %r, data: %r', query, params)
		try:
			with closing(self._db.execute(query, params, **kwz)) as c: yield c
		finally:
			self._db_seq, ts = self._db_seq + 1, time.time()
			if (self._db_ts_limit and (ts - self._db_ts) >= self._db_ts_limit)\
					or (self._db_seq_limit and self._db_seq >= self._db_seq_limit):
				self._db.commit()
				self._db_seq = 0
			self._db_ts = ts

	def _query(self, *query_argz, **query_kwz):
		with self._cursor(*query_argz, **query_kwz): pass

	def _init_db(self):
		with self._db as db: db.executescript(self._db_init)
		with self._cursor("SELECT val FROM meta WHERE var = 'schema_version' LIMIT 1") as c:
			row = c.fetchone()
			schema_ver_old = schema_ver = int(row['val']) if row else 0
		if len(self._db_migrations) > schema_ver:
			for schema_ver, query in enumerate(
				self._db_migrations[schema_ver:], schema_ver+1 ): db.executescript(query)
			self._query( '''INSERT INTO meta (var, val)
				 VALUES ('schema_version', '{}')'''.format(schema_ver) )


	def cleanup(self, ts=None):
		if random.random() > self.conf.history_cleanup_chance: return
		if not ts: ts = time.time()
		ts_cutoff = ts - self.conf.history_timeout
		self._query('DELETE FROM rate_limit_hits WHERE ts < ?', (ts_cutoff,))
		self._query('DELETE FROM rate_limit_state WHERE ts_sync < ?', (ts_cutoff,))

	def tb_state_get(self, sig):
		with self._cursor(
				'''SELECT tokens, ts_sync  FROM rate_limit_state
					WHERE agg_name = ? AND fingerprint = ?''',
				(sig.aggregate_name, sig.fingerprint) ) as c:
			row = c.fetchone()
			if not row: raise LookupError
			return tuple(row)

	def tb_state_set(self, sig, tokens, ts_sync):
		self._query(
			'INSERT OR REPLACE INTO rate_limit_state'
				' (agg_name, fingerprint, tokens, ts_sync) VALUES (?, ?, ?, ?)',
			(sig.aggregate_name, sig.fingerprint, tokens, ts_sync) )
		self.cleanup()
		return tokens, ts_sync

	def digest_get(self, span, count_filter=None, ts=None):
		if not ts: ts = time.time()
		ts_cutoff = ts - span
		having = '' if not count_filter else 'HAVING {} > 0'.format(count_filter)
		with self._cursor(
				'''SELECT
						fingerprint, agg_name,
						COUNT(CASE WHEN pass = 1 THEN 1 ELSE NULL END) AS passed,
						COUNT(CASE WHEN pass = 0 THEN 1 ELSE NULL END) AS filtered
					FROM rate_limit_hits'
					' WHERE ts > ? GROUP BY fingerprint || agg_name {}'''.format(having),
				(ts_cutoff,) ) as c:
			return c.fetchall()

	def digest_add_hit(self, sig, verdict, ts=None):
		if not ts: ts = time.time()
		self._query(
			'INSERT INTO rate_limit_hits'
				' (agg_name, fingerprint, pass, ts) VALUES (?, ?, ?, ?)',
			(sig.aggregate_name, sig.fingerprint, verdict, ts) )
		self.cleanup()


class EEASMailSig(namedtuple('EEASMailSig', 'aggregate_name fingerprint')):

	def __new__(cls, aggregate_name, fingerprint=None, **kws):
		fingerprint = hashlib.sha224(BTE.bencode(fingerprint)).hexdigest()
		return super(EEASMailSig, cls).__new__(cls, aggregate_name, fingerprint)

class EEASMailHandler(object):

	def __init__(self, conf, verdict_default=True):
		self.conf, self.log = conf, log # so that it can easily be used in conf-script
		self.last_verdict = conf.default_verdicts.get(verdict_default, verdict_default)
		self._rate_limit_iter_cache = dict()
		self._db = EEASSigDB(conf, log=log)


	def rate_limit_iter(self, sig, tmin, burst, interval):
		k = sig, burst, interval
		if k not in self._rate_limit_iter_cache:
			self._rate_limit_iter_cache[k] =\
				self._rate_limit_iter(sig, tmin=tmin, burst=burst, interval=interval)
			next(self._rate_limit_iter_cache[k])
		return self._rate_limit_iter_cache[k]

	def _rate_limit_iter(self, sig, tmin, burst, interval):
		try: tokens, ts_sync = self._db.tb_state_get(sig)
		except LookupError: tokens, ts_sync = burst, time.time()
		rate = interval**-1
		val = yield
		while True:
			ts = time.time()
			ts_sync, tokens = ts, min(burst, tokens + (ts - ts_sync) * rate)
			tokens = max(0, tokens - val)
			val = None if tokens >= tmin else ((tmin + val - tokens) / rate)
			tokens, ts_sync = self._db.tb_state_set(sig, tokens, ts_sync)
			val = yield val


	def signature_from(self, **sig_kws): return EEASMailSig(**sig_kws)

	def rate_limit_filter(self, sig, tmin, burst, interval, verdicts=None, grab=1):
		verdicts = verdicts or self.conf.default_verdicts
		verdict = self.rate_limit_iter(sig, tmin, burst, interval).send(grab) is None
		self._db.digest_add_hit(sig, verdict)
		self.last_verdict = verdicts[verdict]

	def mail_pass(self, sig, verdict=None):
		self.last_verdict = verdict or self.conf.default_verdicts[True]
	def mail_filter(self, sig, verdict=None):
		self.last_verdict = verdict or self.conf.default_verdicts[False]


def _mail_header_decode_part(line):
	return ' '.join( val.decode(enc or 'utf-8', 'replace')
		for val, enc in email.header.decode_header(line) )

def mail_header_decode(val):
	res, header = list(), _mail_header_decode_part(val)
	while True:
		match = re.search('=\?[\w\d-]+(\*[\w\d-]+)?\?[QB]\?[^?]+\?=', header)
		if not match:
			res.append(header)
			break
		start, end = match.span(0)
		match = header[start:end]
		try: match = _mail_header_decode_part(match)
		except email.errors.HeaderParseError: pass
		res.extend([header[:start], match])
		header = header[end:]
	return ''.join(res)

def _mail_parse(msg):
	headers = MailMsgHeaders((k.lower(), mail_header_decode(v)) for k,v in msg.items())
	payload = msg.get_payload(decode=True)\
		if not msg.is_multipart() else map(_mail_parse, msg.get_payload())
	if not headers.get('content-type'): headers['content-type'] = [msg.get_content_type()]
	if headers.get_core('content-disposition') == 'attachment': payload = u'<attachment scrubbed>'
	elif isinstance(payload, bytes): payload = payload.decode('utf-8', 'xmlreplace')
	return MailMsg(headers, payload)

def mail_parse(msg_str):
	msg = email.message_from_string(msg_str)
	return _mail_parse(msg)


class MailMsg(namedtuple('MailMsg', 'headers payload')):

	@property
	def all_parts(self):
		return [self] if isinstance(self.payload, unicode)\
			else sorted(it.chain.from_iterable(m.all_parts for m in self.payload), key=len)

	def _text_ct_prio(self, part):
		ct = part.headers.get('content-type')
		if ct == 'text/plain': return 1
		if ct.startswith('text/'): return 2
		return 3

	@property
	def text(self):
		return sorted(self.all_parts, key=self._text_ct_prio)[0].payload

class MailMsgHeaders(OrderedDict):

	def __init__(self, headers_list):
		super(MailMsgHeaders, self).__init__()
		for k, v in headers_list:
			if k not in self: self[k] = list()
			self[k].append(v)

	def get(self, k, default=None, _proc=op.itemgetter(0)):
		hs = super(MailMsgHeaders, self).get(k)
		if not hs: return default
		return _proc(hs)

	def get_core(self, k, default=None):
		return self.get(k, default, lambda hs: hs[0].split(';', 1)[0].strip())

	def get_all(self, k, default=None):
		return self.get(k, default, lambda x: x)


def main(args=None):
	import argparse
	parser = argparse.ArgumentParser(
		description='Email filtering hook to aggregate'
			' and keep track of repeated event notification emails.')

	parser.add_argument('-c', '--conf',
		action='append', metavar='path',
		help='Path(s) to configuration file (python code).'
			' Default is to use first existing one of these: {}'.format(', '.join(Defs.conf_paths)))
	parser.add_argument('-d', '--debug', action='store_true', help='Verbose operation mode.')

	cmds = parser.add_subparsers(
		dest='cmd', title='Actions',
		description='Supported actions (have their own suboptions as well)')


	cmd = cmds.add_parser('check',
		description='Check email vs defined rules and output verdict.',
		epilog='Full message should be piped to stdin, verdict gets printed to stdout.')

	cmd.add_argument('tag', nargs='*',
		help='Tag of a mail processing group to use.'
			' With no tags specified, all groups will be matched.')

	cmd.add_argument('-s', '--max-bytes',
		type=int, metavar='bytes', default=2*2**20,
		help='Max size of emails to process. See also --max-bytes-verdict.')
	cmd.add_argument('--max-bytes-verdict',
		metavar='word', default=Defs.default_verdicts[True],
		help='Vertdict to print (any string) for emails'
			' over --max-bytes in length (default: %(default)r).')

	cmd.add_argument('-e', '--error-reports-dir', metavar='path',
		help='Path to store mails and error reports for processing these to.'
			' Should probably have fairly restricted access permissions, for obvious reasons.')


	cmd = cmds.add_parser('digest',
		description='Output counters of rate-limited messages for a specified period of time.')

	cmd.add_argument('span', nargs='?', default=bytes(Defs.history_timeout),
		help='Timespan (from now)  either as a number'
			' of seconds or in short form with units (e.g. "30s", "10min", "1h 20m", "1mo2w1d", etc).'
			' Default: %(default)ss (history_timeout value).')

	cmd.add_argument('-p', '--if-passed', action='store_true',
		help='Dont output results where none of the mails were passed.')
	cmd.add_argument('-f', '--if-filtered', action='store_true',
		help='Dont output results where none of the mails were filtered.')
	cmd.add_argument('-t', '--table', action='store_true', help='Output results as an rst table.')
	cmd.add_argument('-e', '--use-exit-code', action='store_true',
		help='Exit with non-zero status code if there was any output.'
			' Default is to always exit with success status.')


	opts = parser.parse_args(sys.argv[1:] if args is None else args)

	global log
	import logging
	log_fmt = logging.Formatter(
		'%(asctime)s :: %(levelname)s :: %(message)s', '%Y-%m-%d %H:%M:%S' )
	log_handler = logging.StreamHandler(sys.stderr)
	log_handler.setLevel(logging.DEBUG if opts.debug else logging.WARNING)
	log_handler.setFormatter(log_fmt)
	logging.root.addHandler(log_handler)
	logging.root.setLevel(0)
	log = logging.getLogger()

	report_handler = None
	if getattr(opts, 'error_reports_dir', None):
		from logging.handlers import BufferingHandler
		report_handler = BufferingHandler(capacity=1e4)
		report_handler.setFormatter(log_fmt)
		report_handler.buffer = deque(maxlen=report_handler.capacity)
		report_handler.capacity += 1
		report_handler.setLevel(0)
		logging.root.addHandler(report_handler)

	conf_paths = map(expanduser, opts.conf or Defs.conf_paths)
	for p in conf_paths:
		if exists(p): break
	else:
		parser.error( 'Failed to find configuration file'
			' (tried paths: {})'.format(', '.join(map(repr, conf_paths))) )
	log.debug('Using configuration path: %r', p)
	ns0 = dict(
		it=it, op=op, ft=ft, re=re, types=types, string=string,
		db_path=expanduser(Defs.db_path),
		default_verdicts=Defs.default_verdicts,
		history_timeout=Defs.history_timeout,
		history_cleanup_chance=Defs.history_cleanup_chance,
		mail_max_bytes=getattr(opts, 'max_bytes', None),
		mail_max_bytes_verdict=getattr(opts, 'max_bytes_verdict', None) )
	with open(p, 'rb') as src: code = compile(src.read(), '<string>', 'exec')
	exec code in ns0
	ns = type('Namespace', (object,), dict())()
	ns.__dict__.update(ns0)

	if opts.cmd == 'digest':
		if not opts.if_passed ^ opts.if_filtered: count_filter = None
		elif opts.if_passed: count_filter = 'passed'
		elif opts.if_filtered: count_filter = 'filtered'

		span = parse_timedelta(opts.span)
		db = EEASSigDB(ns, log=log)
		digest = db.digest_get(span, count_filter)
		if not digest: return

		agg_name_len = max(len(line['agg_name']) for line in digest)
		num_len = len(bytes(max(
			map(op.itemgetter('passed'), digest)
			+ map(op.itemgetter('filtered'), digest) )))

		if opts.table:
			table_head = dict(agg_name='name', passed='passed', filtered='filtered', total='total')
			table_head_len = max(map(len, table_head.values()))
			agg_name_len, num_len = max(agg_name_len, table_head_len), max(num_len, table_head_len)
			table_hr = lambda s: print('{0} {1} {1} {1}'.format(s*(agg_name_len + 2), s*(num_len + 2)))
			line_fmt = lambda head=False:\
				' {{0[agg_name]:<{0}s}}   {{0[passed]:✗}}   {{0[filtered]:✗}}   {{1:✗}}'\
				.replace('✗', '>{1}d' if not head else '^{1}s').format(agg_name_len, num_len)
			table_hr('=')
			print(line_fmt(True).format(table_head, table_head['total']))
			table_hr('-')
			for line in digest:
				print(line_fmt().format(line, line['passed'] + line['filtered']))
			table_hr('=')

		else:
			line_fmt =\
				'{{0[agg_name]:>{0}s}} :: passed={{0[passed]:✗}} filtered={{0[filtered]:✗}}'\
				.replace('✗', '>0{1}d').format(agg_name_len, num_len)
			for line in digest: print(line_fmt.format(line))

		if opts.use_exit_code: return 1

	elif opts.cmd == 'check':
		msg_str = sys.stdin.read(ns.mail_max_bytes + 1)
		if len(msg_str) > ns.mail_max_bytes: verdict = ns.mail_max_bytes_verdict
		else:
			log.debug('Processing message (%sB)', len(msg_str))
			try:
				msg = mail_parse(msg_str)
				eeas = EEASMailHandler(ns)
				ns.parser(eeas, opts.tag, msg)
				verdict = eeas.last_verdict
			except Exception as err:
				log.exception('Failed to parse/classify email message: %s', err)
				if not opts.error_reports_dir: return 1
				fn_base = join(opts.error_reports_dir, '{}_[{}]'.format(
					time.strftime('%Y%m%d_%H%M%S'), base64.urlsafe_b64encode(os.urandom(3)) ))
				if not exists(opts.error_reports_dir): os.makedirs(opts.error_reports_dir)
				with open('{}.mail'.format(fn_base), 'wb') as dst: dst.write(force_bytes(msg_str))
				with open('{}.log'.format(fn_base), 'wb') as dst:
					if report_handler:
						for line in report_handler.buffer:
							line = report_handler.format(line)
							dst.write('{}\n'.format(force_bytes(line)))
				return 1
			log.debug('Processing verdict: %s', verdict)
		sys.stdout.write(verdict)

if __name__ == '__main__': sys.exit(main())
