
aggregation:
  # Parameters for the token-bucket algorithm
  # Can be overidden on a per-group basis below

  # Each matched mail "grabs" a token, or a fraction
  #  of one, regardless of how it gets classified in the end
  # burst=5 means "max 5 tokens in a bucket"
  burst: 3

  # When number of tokens drops below "min", stuff gets rate-limited
  # Note that number of tokens can be fractional, so that if mails hit bucket
  #  with interval=1d more than 1/d, there will always be 0 <= n < 1 tokens,
  #  so with min=1, nothing will pass, until rate drops below 1/d
  min: 1

  # Interval between new tokens
  # Examples: "30s", "10min", "1h 20m", "1mo 10h"
  # All units: y (yr, year), mo (month), w (week),
  #  d (day), h (hr, hour), m (min, minute), s (sec, second)
  interval: 3d


groups:

  cron_jobs_without_timestamps: # arbitrary processing group name

    # This group would be applied to message if
    #  any of the group tags will be specified on the command line
    tags:
      - cron-jobs
      - from-myhost

    max-length: 5_000_000 # mails with attachments aren't interesting anyway

    # aggregation: # same keys as in global section

    #
    # "parser" is a py function that should accept a following data structure:
    #
    #   group: cron_jobs_without_timestamps
    #   tags:
    #     group: [group-tag-1, ...]
    #     cli: [cli-tag-1, ...]
    #   headers:
    #     - name: header-name-1
    #       value: decoded-header-1
    #     ...
    #   body:
    #     - mime: mime-type-1
    #       data: decoded-body-part-1
    #     ...
    #
    # (input data might have additional keys in the future)
    # ...and return a dict like this:
    #
    #   aggregate_name: 'root@myhost my-noisy-script'
    #   fingerprint: 857b9c903af724a42d7f
    #
    # Where "aggregate-name" will be only printed in a digest
    #  to show how many mails with this fingerprint were rate-limited.
    # None can also be returned for "does-not-match" kind of mails.
    #
    # Script will be run in a clean-ish namespace, and should define
    #  "parser" function in it, which will then be used as outlined above.
    # Idea here is to have powerful and flexible logic without any extra DSLs.
    #
    parser: |
      import re
      def parser(data):
        agg_name = fingerprint = None
        for header in headers:
          if header['name'].lower() != 'subject': continue
          m = re.search( r'^Cron\s+'
            r'<(?P<src>[^>]+)>\s+(?P<name>.*)$', header['value'] )
          if not m: continue
          agg_name = '{} {}'.format(m.group('src'), m.group('name'))
        if not agg_name: return
        for part in body:
          if not part['mime'].startswith('text/'): continue
          '...calculate-some-fingerprint...'
        return dict(aggregate_name=agg_name, fingerprint=fingerprint)

  # my-whatever-other-event-group: ...
